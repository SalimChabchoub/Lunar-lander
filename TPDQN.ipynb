{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP1 Deep QLearning\n",
    "\n",
    "Dans ce TP, l'objectif est d'implémenter un agent apprenant à faire atterir un vaisseau sur la lune avec l'algorithme Deep Q-Network. Pour cela vous allez utiliser [PyTorch](https://pytorch.org/) et [Gymnasium](https://gymnasium.farama.org/). \n",
    "\n",
    "<img src='img/lunarlander.png'  width=500px>\n",
    "\n",
    "# 1. Consignes\n",
    "\n",
    "> <span style=\"color:red\">Compléter ce notebook et les différents fichiers python associés</span>.  \n",
    "> \n",
    ">  <span style=\"color:red\">Répondre aux questions</span> (dans les cellules <span style=\"color:blue\">Votre réponse: </span> )\n",
    "\n",
    "\n",
    "> Le code doit être fonctionnel avec l'environnement virtuel du TP. Si d'autres packages que ceux présents dans l'environnement virtuel créé au départ sont nécessaires, vous devez ajouter à votre dépôt un fichier `environnement.yaml` qui est un export de votre environnement virtuel. Ce fichier est obtenu avec la commande suivante:  ```conda env export > environnement.yaml```\n",
    "\n",
    "\n",
    "\n",
    "# 2. Import des packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import random\n",
    "import torch\n",
    "import numpy as np\n",
    "from collections import deque\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "\n",
    "def init_seed(seedval):\n",
    "    torch.manual_seed(seedval)\n",
    "    np.random.seed(seedval)\n",
    "    random.seed(seedval)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Gymnasium\n",
    "\n",
    "En apprentissage par renforcement, il y a deux concepts fondamentaux : l’agent et l’environnement.\n",
    "- L’agent est l’entité apprenante qui observe l’environnement et agit sur celui-ci selon les actions disponibles. Son objectif est de maximiser la récompense cumulée qu’il recoit de l’environnement avec lequel il interagit.\n",
    "- L'agent interagit avec l'environnement à travers la boucle de perception/action ce qui nécessite de définir :\n",
    "    - Un espace d’action.\n",
    "    - Un espace d’état (ou observation).\n",
    "    - Une fonction de récompense.\n",
    "  \n",
    "[Gymnasium](https://gymnasium.farama.org/) propose une interface open source unifiée entre un agent et un environnement.\n",
    "- [Gymnasium](https://gymnasium.farama.org/) propose un ensemble d'environnements pour des tâches d'apprentissage par renforcement. La plupart des environnements ont leur code source disponible sur [GitHub](https://github.com/Farama-Foundation/Gymnasium/tree/main/gymnasium/envs). De nouveaux environnements peuvent aussi être créés à condition qu'ils soient compatibles avec l'interface. \n",
    "- Grâce à l'interface unifiée, il est possible de définir indépendamment un agent de l’environnement avec lequel il interagit (et inversement). \n",
    "- Lorsque certains pré-traitements sont nécessaires sur les actions, observations, récompenses, ... il est possible d’encapsuler l’environnement dans un **wrapper**, celui-ci se chargera du pré-traitement. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    ">  <span style=\"color:green\">Documentation de Gymnasium</span>: [utilisation basique](https://gymnasium.farama.org/content/basic_usage/), [API pour les environnements](https://gymnasium.farama.org/api/env/), ...\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  3.1 - Caractéristiques de l'environnement LunarLander\n",
    "\n",
    "Dans ce TP, nous allons implémenter un agent qui interagira avec l'environnement **LunarLander**, présenté  [ici](https://gymnasium.farama.org/environments/box2d/lunar_lander/) et le code source est [ici](https://github.com/openai/gym/blob/master/gym/envs/box2d/lunar_lander.py). 2 versions de LunarLander existent: avec des actions discrètes ou des actions continues.\n",
    "\n",
    "> <span style=\"color:green\">Quelle version choisir pour utiliser le DQN ? Pourquoi ? \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "Dans notre cas, on utilisera la version avec des actions discrètes puisqu'on aura besoin pour estimer la valeur Q pour chaque action possible à partir d'un état donné. Pour utiliser le DQN, il faut alors choisir un environements à espaces d'actions discret ou finis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Il existe plusieurs fonctions clés pour interagir avec un environnement Gymnasium ( [utilisation basique](https://gymnasium.farama.org/introduction/basic_usage/) et [API](https://gymnasium.farama.org/api/env/)).\n",
    "\n",
    "\n",
    "> <span style=\"color:green\">Compléter la cellule de code ci-dessous pour afficher:\n",
    "> \n",
    "> - **les dimensions pour les espaces d'états et d'actions** de l'environnement `LunarLander`.\n",
    "> - les bornes min et max pour les dimensions de l'état\n",
    "> - un échantillon pris au hasard dans chaque espace. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dimensions pour les espaces d'actions: Discrete(4)\n",
      "Dimensions pour les espaces d'états: (8,)\n",
      "Les bornes max des états: [ 2.5        2.5       10.        10.         6.2831855 10.\n",
      "  1.         1.       ]\n",
      "Les bornes min des états: [ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
      "  -0.         -0.       ]\n",
      "Un échantillon aléatoire de l'espace d'états: [-1.6287723  -0.7433806  -5.211418   -7.730384    4.713408   -0.05818327\n",
      "  0.81629986  0.97619945]\n"
     ]
    }
   ],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "#TODO\n",
    "print(\"Dimensions pour les espaces d'actions: \" + str(env.action_space))\n",
    "print(\"Dimensions pour les espaces d'états: \" + str(env.observation_space.shape))\n",
    "print(\"Les bornes max des états: \" + str(env.observation_space.high))\n",
    "print(\"Les bornes min des états: \" + str(env.observation_space.low))\n",
    "print(\"Un échantillon aléatoire de l'espace d'états: \" + str(env.observation_space.sample()))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\"> </span>  A quoi correspond dans le cas du LunarLander une terminaison d'un épisode avec *terminated* à True ? avec *truncated* à True ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>\n",
    "On termine un épisode avec *terminated* à True si:\n",
    "- Le vaisseau s'écrase (le corps du vaisseau touche la surface de la lune)\n",
    "- Le vaisseau sort du cadre de l'interface (les coordonnée de x dépasse 1)\n",
    "- Le vaisseau n'est plus réveiller. D'après les docs de Box2D, un coprs qui n'est réveillé est un corps qui ne bouge plus et qui ne touche aucune surface.\n",
    "\n",
    "On termine un épisode avec *trancated* à True si un épisode est terminé à cause des contraintes externes comme la limite du temps, la limite des pas d'apprentissage, etc..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Interaction et affichage de l'environnement sur un épisode\n",
    "\n",
    "> <span style=\"color:green\">Exécuter une instance de l'environnement `LunarLander` pendant un épisode avec des **actions aléatoires**. Afficher l'environnement à chaque pas pour visualiser le comportement du vaisseau. A la fin de l'épisode, afficher la somme des récompenses obtenues sur l'épisode et la raison pour laquelle l'épisode s'est terminé. </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "#TODO\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Agent simple (sans apprentissage)\n",
    "\n",
    "Vous devez maintenant implémenter un **agent simple** qui utilise une **Q fonction paramétrée** (réseau de neurones) pour choisir ses actions selon une stratégie $\\epsilon$-greedy (mais pas d'apprentissage pour l'instant).\n",
    "\n",
    "> <span style=\"color:green\">- Quels éléments seront en entrée du NN ? En sortie ? Quelle sera la dimension de l'entrée ? De la sortie ? </span>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">- Quelle fonction d'activation sera utilisée sur le dernière couche du réseau ? Pourquoi ? </span>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- définir l'architecture du réseau de neurones en complétant `QNN.py`. Ce réseau va approximer la Q-fonction comme dans DQN (pour l'instant, les poids du réseau ne seront pas mis à jour, le réseau est uniquement utilisé en prédiction).\n",
    "\n",
    "- définir un agent simple en complétant `agentsimple.py`: il utilisera la prédiction du réseau de neurones pour choisir ses actions selon une stratégie d’exploration $\\epsilon$-greedy.\n",
    "\n",
    "- utiliser cet agent sur plusieurs épisodes dans `LunarLander`. Vous utiliserez une décroissance de l'exploration, i.e. que $\\epsilon$ va décroitre à chaque épisode, en démarrant à une valeur élevée (beaucoup d'exploration) et avec une borne minimum. Ainsi, au premier épisode, $\\epsilon=\\epsilon_{start}$, et à chaque épisode, $\\epsilon=max(\\epsilon_{end}, \\epsilon_{decay}*\\epsilon)$. Par exemple sur 1000 épisodes, les valeurs peuvent être $\\epsilon_{start} = 1.0$, $\\epsilon_{end} = 0.01$ et $\\epsilon_{decay} = 0.995$.\n",
    "\n",
    "- proposer un tracé de la somme des récompenses obtenues par épisode (vous pouvez utiliser le fichier `utils.py`, le wrapper [RecordEpisodeStatistics](https://gymnasium.farama.org/api/wrappers/misc_wrappers/#gymnasium.wrappers.RecordEpisodeStatistics ),  ...).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules every time before executing the Python code typed\n",
    "%autoreload 2\n",
    "# import depuis un fichier python local \n",
    "from QNN import QNN \n",
    "from agentsimple import AgentSimple\n",
    "import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Deep QLearning avec Replay Buffer\n",
    "\n",
    "L'algorithme (sans réseau cible) est donné ci-dessous:\n",
    "\n",
    "<img src='img/DQN2.png'  width=700px>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- La phase d'**échantillonage** est une phase d'interaction avec l'environnement (sans apprentissage), pendant laquelle l'agent stocke en mémoire toutes les transitions rencontrées. Une transition est un tuple `(état,action,état_suivant,récompense,fin_episode)`. La mémoire a une taille maximale; lorsqu’elle est dépassée, les nouvelles transitions remplacent les plus anciennes. \n",
    "\n",
    "- La phase d'**apprentissage** permet de mettre à jour les paramètres de la fonction *Q* à partir de plusieurs (minibatch) transitions (64 par exemple) stockées dans la mémoire. La phase d'apprentissage est réalisée tous les `t` pas (ou actions) dans l'environnement (par ex. $t=4$).\n",
    "\n",
    "- La classe `ReplayBuffer` du fichier `replaybuffer.py` permet de stocker des transitions dans une mémoire et de récupérer des minibatch de transitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Vous devez implémenter l'algorithme du **Deep QLearning avec ReplayBuffer** donné ci-dessus.\n",
    "\n",
    "Remarque: \n",
    "- vous n'utiliserez pas de réseau cible (*target network*) pour l'instant.\n",
    "- Pour l'optimizer, SGD et [Adam](https://machinelearningmastery.com/adam-optimization-algorithm-for-deep-learning/) sont particulièrement adaptés. \n",
    "- Dans la phase d'apprentissage, les éléments récupérés dans un minibatch  sont des tenseurs de taille $d \\times dim\\_element$. Vous devez dans cette phase faire des calculs tensoriel directement (et pas de boucle for sur $d$ !)\n",
    "- Voici aussi des liens vers différentes fonctions de PyTorch qui pourraient vous être utiles: [unsqueeze](https://pytorch.org/docs/stable/generated/torch.unsqueeze.html), [gather](https://pytorch.org/docs/stable/generated/torch.gather.html)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">Compléter la classe `AgentDQN`</span> \n",
    ">\n",
    "> <span style=\"color:green\">Implémenter l'algorithme dans la fonction *dqnalgo* ci-dessous.  </span> \n",
    ">\n",
    "> <span style=\"color:green\">Utiliser cet agent sur plusieurs épisodes dans l'environnement `LunarLander`. Vous proposerez un tracé de la somme des récompenses obtenues par épisode (vous pouvez utiliser `utils.py`).</span> \n",
    "\n",
    "\n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Reload all modules every time before executing the Python code typed\n",
    "%autoreload 2\n",
    "from replaybuffer import ReplayBuffer\n",
    "from agentdqn import AgentDQN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO\n",
    "\n",
    "def dqnalgo(agent,env,nb_episodes,eps_start,eps_end,eps_decay):\n",
    "    \"\"\"\n",
    "        Retourne la somme des recompenses par épisode\n",
    "    \"\"\"\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\")\n",
    "\n",
    "#TOCOMPLETE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\">Faites maintenant une moyenne sur différents seed, avec un apprentissage par seed, et tracer le résultat (somme des récompenses par épisodes) moyenné sur ces seed. Vous utiliserez `init_seed()` et pouvez utiliser `utils.py`.</span> </span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Deep QLearning avec réseau cible\n",
    "\n",
    "Il se peut que votre agent précédent apprenne des comportements intéressants mais qu’ils soient très instables. On va maintenant ajouter un réseau cible pour l'améliorer.\n",
    "\n",
    "> <span style=\"color:green\"> Quelle est la cause des instabilités de l'algorithme précédent ? Pourquoi le réseau cible améliore ces instabilités ?</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "> <span style=\"color:blue\">Votre réponse: </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L'algorithme DQN (avec réseau cible) est donné ci-dessous:\n",
    "\n",
    "<img src='img/DQNcible.png'  width=500px>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Le réseau cible sera mis à jour toutes les N étapes d'apprentissage (500 par exemple si $t=4$) en recopiant entièrement le réseau de neurone original dans le duplicat.\n",
    "\n",
    "Pour copier des poids d'un réseau de neurone vers un autre, la méthode `copy_` peut être appelée sur les paramètres:\n",
    "\n",
    "`for param_duplicat, param_source in zip(model_duplicat.parameters(), model_source.parameters()):`\n",
    "           \n",
    "`param_duplicat.data.copy_(param_source.data)`\n",
    "\n",
    "> <span style=\"color:green\"> Compléter la classe `AgentDQNCible` pour implémenter un agent apprenant avec DQN (deep QLearning et *target network*).</span> \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "> <span style=\"color:green\">Utiliser cet agent **dans l'algorithme dqnalgo précédent**. Vous proposerez un tracé de la somme des récompenses obtenues par épisode sur plusieurs seed.</span> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from agentdqncible import AgentDQNCible"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Sauvegarde d'un agent\n",
    "\n",
    "> <span style=\"color:green\"> Sauvegarder un agent qui a correctement appris dans un fichier 'monAgentDQN.pth'. Vous préciserez ci-dessous les hyperparamètres utilisés pour cet agent. </span> \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Hyperparamètres de l'agent: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Exemple de code pour sauvegarde d'un réseau\n",
    "#savedfile = 'checkpoint.pth'\n",
    "#torch.save(agent.network.state_dict(), savedfile)\n",
    "\n",
    "#Exemple de code pour chargement d'un reseau sauvegarde\n",
    "#state_dict = torch.load(savedfile)\n",
    "#agent.network.load_state_dict(state_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\"> Proposer un code ci-dessous pour visualiser un épisode de cet agent en mode glouton. </span> \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"LunarLander-v3\", render_mode=\"human\")\n",
    "#TODO "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:green\"> Si vous avez testé différents hyperparamètres, vous pouvez le préciser ci-dessous. </span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> <span style=\"color:blue\">Hyperparamètres testés: </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tpdeeprl2024",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
